{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,os,json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# finds the root of a word (any tense it may be in)\n",
    "stemmer = PorterStemmer()\n",
    "# to find the actual root word\n",
    "lemmater = WordNetLemmatizer()\n",
    "# stop words which carry no real meaning in a bag of words representation\n",
    "stopper = stopwords.words('english') + [\"he\", \"she\", \"his\",\"it\",\"is\"]\n",
    "# regular expression containg useless punctuations\n",
    "punc = re.compile(r'''[\",.<>/();:'-_=?!|0-9]''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_vocab = \"imdb_vocab.json\" in os.listdir()\n",
    "if_trained = \"imdb.pth\" in os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"imdd.csv\")\n",
    "data = shuffle(data)\n",
    "\n",
    "labels = np.array(data[\"sentiment\"])\n",
    "features = np.array(data[\"review\"])\n",
    "\n",
    "if not if_trained:\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        # print(len(features[i]))\n",
    "        # print(features[i][:155])\n",
    "        for word in stopper:\n",
    "            features[i] = features[i].replace(\" \"+word+\" \",\" \")\n",
    "            features[i] = features[i].replace(\"<br />\",\" \").replace(\"<br/>\",\" \")\n",
    "        # print(len(features[i]))\n",
    "        # print(features[i][:155])\n",
    "\n",
    "        features[i] = punc.sub(\"\",features[i])\n",
    "        # print(len(features[i]))\n",
    "        # print(features[i][:155])\n",
    "\n",
    "        tokens = word_tokenize(features[i])\n",
    "        # print(len(tokens))\n",
    "        # print(tokens[:15])\n",
    "\n",
    "        for j in range(len(tokens)):\n",
    "            if \"http\" in tokens[j]:\n",
    "                tokens[j] = \"\"\n",
    "            tokens[j] = lemmater.lemmatize(tokens[j])\n",
    "            # tokens[j] = stemmer.stem(tokens[j])\n",
    "        \n",
    "        features[i] = np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10059"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not if_vocab:\n",
    "    vocab = {}\n",
    "\n",
    "    for tokens in features:\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                vocab[token]+=1\n",
    "            except:\n",
    "                vocab[token]=1\n",
    "\n",
    "    i = 0\n",
    "    for word in list(vocab.keys()):\n",
    "        if vocab[word] <= 52:\n",
    "            vocab.pop(word)\n",
    "        else:\n",
    "            vocab[word] = i\n",
    "            i+=1\n",
    "    with open(\"imdb_vocab.json\", \"w\") as file:\n",
    "        json.dump(vocab,file)\n",
    "else:\n",
    "    with open(\"imdb_vocab.json\", \"r\") as file:\n",
    "        vocab = json.load(file)\n",
    "        print(\"Vocabulary Loaded\")\n",
    "\n",
    "\n",
    "vocab_len = len(vocab)\n",
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 10059])\n"
     ]
    }
   ],
   "source": [
    "if not if_trained:\n",
    "    final_features = torch.zeros(50000,vocab_len, dtype=torch.float32, requires_grad=False)\n",
    "    print(final_features.shape)\n",
    "    for i in range(len(features)):\n",
    "        for j in range(len(features[i])):\n",
    "            index = vocab.get(features[i][j],None)\n",
    "            if index:\n",
    "                final_features[i][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 1])\n"
     ]
    }
   ],
   "source": [
    "if not if_trained:\n",
    "    final_labels = torch.zeros(50000,1, dtype=torch.float32, requires_grad=False)\n",
    "    print(final_labels.shape)\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == \"positive\":\n",
    "            final_labels[i] = 1\n",
    "        else:\n",
    "            final_labels[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(vocab_len,1000),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(1000,10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10,1),\n",
    "    torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Function and Optimizer created\n"
     ]
    }
   ],
   "source": [
    "if not if_trained:\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
    "    print(\"Loss Function and Optimizer created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders Created\n"
     ]
    }
   ],
   "source": [
    "if not if_trained:\n",
    "    BATCH_SIZE = 200\n",
    "\n",
    "    dataset = TensorDataset(final_features[:45000], final_labels[:45000])\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(final_features[45000:], final_labels[45000:])\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    print(\"Dataloaders Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\n",
      "Loss : 0.2801854610443115\n",
      "Accuracy : 93.9\n",
      "Epoch : 2\n",
      "Loss : 0.24062980711460114\n",
      "Accuracy : 97.6\n",
      "Epoch : 3\n",
      "Loss : 0.08529292047023773\n",
      "Accuracy : 99.45\n",
      "Epoch : 4\n",
      "Loss : 0.04041421413421631\n",
      "Accuracy : 99.66\n",
      "Epoch : 5\n",
      "Loss : 0.00128135085105896\n",
      "Accuracy : 99.74\n"
     ]
    }
   ],
   "source": [
    "if not if_trained:\n",
    "    epochs = 5\n",
    "else:\n",
    "    epochs = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for f,ls in dataloader:\n",
    "        preds = model(f)\n",
    "        loss = loss_fn(preds,ls)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        right = 0\n",
    "        for f,ls in test_dataloader:\n",
    "            preds = model(f)\n",
    "            \n",
    "            for i in range(len(preds)):\n",
    "                if (preds[i] >= 0.5) == ls[i]:\n",
    "                    right += 1\n",
    "    \n",
    "    # if (epoch+1)%(epochs//10) == 0:\n",
    "    print(f\"Epoch : {epoch+1}\\nLoss : {loss}\\nAccuracy : {round((right*100)/(BATCH_SIZE*len(test_dataloader)),5)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not if_trained:\n",
    "    torch.save(model.state_dict(), \"imdb.pth\")\n",
    "else:\n",
    "    model.load_state_dict(torch.load(\"imdb.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(string):\n",
    "    for word in stopper:\n",
    "        string = string.replace(\" \"+word+\" \",\" \")\n",
    "        string = punc.sub(\"\",string)\n",
    "    \n",
    "    tokens = word_tokenize(string)\n",
    "    \n",
    "    if len(tokens) <= 10:\n",
    "        raise Exception(\"Sentence too small to make prediction\")\n",
    "        \n",
    "    for j in range(len(tokens)):\n",
    "        tokens[j] = lemmater.lemmatize(tokens[j])\n",
    "    print(tokens)\n",
    "    pred_features = torch.zeros(1,vocab_len, dtype=torch.float32)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        index = vocab.get(tokens[i],None)\n",
    "        if index:\n",
    "            # print(tokens[i],end=\" \")\n",
    "            pred_features[0][index] += 1\n",
    "\n",
    "    pred = model(pred_features)\n",
    "    return round(float(pred[0])*100,2)       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['went', 'film', 'high', 'expectation', 'film', 'delivered', 'plenty', 'many', 'time', 'others', 'found', 'quite', 'bored', 'tempted', 'write', 'issue', 'took', 'away', 'star', 'film', 'really', 'fun', 'happy', 'thing', 'everyone', 'love', 'isnt', 'arguing', 'often', 'get', 'that', 'o', 'say', 'worth', 'watch', 'leave', 'that']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95.39"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('''\n",
    "I went into this film with very high expectations. The film delivered plenty at many times, but others I found myself quite bored.\n",
    "\n",
    "I was tempted to write out all the issues I had that took away the 3 stars, but this film is really just a fun, happy thing that everyone loves and isn't arguing about, and how often do we get that?\n",
    "\n",
    "So I'll just say it's worth the watch and leave it at that.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hen', 'weird', 'whacky', 'thing', 'click', 'ou', 'know', 'fun', 'ride', 'matter', 'humour', 'lot', 'like', 'show', 'certain', 'appeal', 'ike', 'fact', 'someone', 'actually', 'returned', 'home', 'isekai', 'ow', 'many', 'name', 'accomplished', 't', '’', 's', 'new', 'creative', 'rt', 'sound', 'he', 'uncle', 'hit', 'miss', 't', '’', 's', 'weird', 'situation', 'quite', 'understandable', 'matter', 'feel', 'choice', 'personality', 'ne', 'fact', 'ou', 'least', 'understand', 'situation', 'he', 'show', 'really', 'wild', 'bag', 'ut', 'interesting', 'premise', 'keep', 'ticking', 'ive', 'go', 'eviewer', '’', 's', 'ating']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('''\n",
    " When weird and whacky things click. You know you are in for a fun ride. No matter, if the humour is for you or not. There is a lot to like about this show, as if it has a certain appeal. Like the fact someone actually returned home from an isekai. How many can you name have accomplished this??? It’s new and creative.\n",
    "\n",
    "Art and sound: 8\n",
    "\n",
    "The uncle can be very hit or miss for me. It’s weird, but his situation is quite understandable. So no matter how you feel on him, or his choices and personality. One fact is. You can least understand his situation.\n",
    "\n",
    "The show really is a wild bag. But it has an interesting premise to keep it ticking and more so. Give it a go.\n",
    "Reviewer’s Rating: 8 \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o', 'see', 'movie', 'you', 'infatuated', 'om', 'ruise', 'love', 'armed', 'force', 'promos', 'love', 'dont', 'know', 'much', 'fighter', 'jet', 'military', 'air', 'operation', 'military', 'general', 'love', 'romantic', 'story', 'dont', 'care', 'fact', 'logic', 'therwise', 'disappointed', 'say', 'least']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('''\n",
    "Go see this movie if you:\n",
    "\n",
    "    are infatuated with Tom Cruise\n",
    "    love US armed forces promos\n",
    "    love, but don't know much about fighter jets, military air operations, or the military in general\n",
    "    love romantic stories\n",
    "    don't care about facts or logic\n",
    "\n",
    "Otherwise, you will be disappointed, to say the least.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'training', 'painfully', 'uninteresting', 'cheesy', 'smiling', 'fine', 'expected', 'creates', 'positive', 'atmosphere', 'viewer', 'also', 'make', 'viewer', 'attached', 'character', 'character', 'trouble', 'later', 'movie', 'he', 'actual', 'mission', 'last', 'third', 'say', 'point', 'probably', 'third', 'term', 'actual', 'portion', 'movie', 'covered', 'ore', 'accurately', 'probably', 'fifth', 'movie', 'le', 'best', 'part', 'movie', 'even', 'great', 'even', 'good', 'mentioned', 'fairly', 'smaller', 'part', 'movie', 'ut', 'beyond', 'scene', 'completely', 'predictable', 'ow', 'something', 'predictable', 'necessarily', 'make', 'bad', 'feeling', 'actionpart', 'movie', 'feel', 'like', 'watching', 'trailer', 'over', 'minute', 'nothing', 'unique', 'entire', 'scene', 'positive', 'aspect', 'boring', 'ne', 'last', 'note', 'storyline', 'almost', 'doesnt', 'exist', 'storyline', 'there', 'mission', 'completed', 'hat', 'n', 'summation', 'movie', 'leaf', 'lot', 'desired']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('''\n",
    "The training is painfully uninteresting and cheesy. The smiling is fine because it's expected and creates a positive atmosphere for the viewer. It also makes the viewer more attached to the character when that character is in trouble later in the movie.\n",
    "\n",
    "The actual mission is the last third and I have the most to say about that. I should point out that it probably was not a third in terms of the actual portion of the movie that it covered. More accurately, it's probably about a fifth of the movie or less. This was the best part of the movie, but even this was not great or even that good. As mentioned, it's a fairly smaller part of the movie. But beyond that, the scenes are completely predictable. Now, just because something is predictable does not necessarily make it bad. The feeling here is that this action-part of the movie feels like watching the trailer over and over for 20-30 minutes. There is nothing that unique about the entire scene. The only positive aspect of it is that it's not boring.\n",
    "\n",
    "One last note, the storyline, it almost doesn't exist. The storyline is that there's a mission that has to be completed. That's it. In summation, the movie leaves a lot to be desired.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'honest', 'wasnt', 'sure', 'watching', 'one', 'actually', 'watching', 'first', 'episode', 'completely', 'changed', 'thought', 'anime', 'continued', 'streaming', 'found', 'unique', 'world', 'building', 'story', 'line', 'progressing', 'smoothly', 'main', 'charactrer', 'actual', 'main', 'character', 'one', 'antigonist', 'different', 'amazing', 'storyline', 'worth', 'watiching', 'conclude', 'whole', 'new', 'experiance', 'dont', 'regret', 'watching']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('''\n",
    "to be honest i wasn't sure about watching this one, but after actually watching the first episode it completely changed my thoughts on this anime. As i continued streaming it, i found out how unique it was, the world building, the story line that was progressing smoothly, and all of the above the main charactrer that was not the actual main character, but was one of them.  the antigonist had a different and amazing storyline which was worth watiching. To conclude, it was a whole new experiance, i dont regret watching. \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['poor', 'cinematograpy', 'drunk', 'actor', 'body', 'showing', 'movie', 'absolute', 'piece', 'trash', 'please', 'dont', 'watch', 'cinema', 'maybe', 'high', 'nothing', 'occupy', 'mind', 'watching', 'this']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('''\n",
    "poor cinematograpy, drunk actors, only body showing through out the movie, absolute piece of trash, please dont watch it in cinemas, maybe when you are high and have nothing to do you can occupy your mind by watching this\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
